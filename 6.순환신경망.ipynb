{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6-1 순환 신경망\n",
        "## 1. 순환 신경망(Recurrent Neural Network, RNN)\n",
        "- 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로 향했는데 이와 같은 신경망들을 피드 포워드 신경망이라고 한다.\n",
        "- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있다.\n",
        "- RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 한다.\n",
        "- 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 메모리 셀 또는 RNN 셀이라고 표현한다.\n",
        "- 은닉층의 메모리 셀은 각각의 시점(tiem step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 한다.\n",
        "- 메모리 셀이 출력층 방향 또는 다음 시점 t+1의 자신에게 보내는 값을 은닉 상태(hidden state)라고 한다.\n",
        "- RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용한다.\n",
        "- 은닉층\n",
        "\n",
        "  $h_t = tanh(W_xx_t + W_hh_{t-1} + b)$\n",
        "- 출력층\n",
        "\n",
        "  $y_t = f(W_yh_t + b)$\n",
        "  단, f는 비선형 활성화 함수 중 하나.\n",
        "- $h_t$를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수가 사용되지만, ReLU로 바꿔 사용하는 시도도 있다.\n",
        "- 각각의 가중치 $W_x, W_h, W_y$의 값은 모든 시점에서 값을 동일하게 공유한다.\n"
      ],
      "metadata": {
        "id": "2Ob5PNu5ThaZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gubWQLM0S-8k",
        "outputId": "4cc56c36-fa2e-45b5-f85f-3af249ec4500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# 2. 파이썬으로 RNN 구현하기\n",
        "import numpy as np\n",
        "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
        "input_size = 4 # 입력의 차원. 보통 단어 벡터의 차원이 된다.\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량\n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
        "hidden_state_t = np.zeros((hidden_size, )) # 초기 은닉 상태는 0(벡터)로 초기화\n",
        "\n",
        "print(hidden_state_t)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = np.random.random((hidden_size, input_size)) # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치\n",
        "b = np.random.random((hidden_size,)) # (8, )크기의 1D 텐서 생성. 편향\n",
        "\n",
        "print(np.shape(Wx))\n",
        "print(np.shape(Wh))\n",
        "print(np.shape(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByIOtwpkXjK9",
        "outputId": "2183802f-e189-4432-b193-8eae9f9941ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "for input_t in inputs:\n",
        "  output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
        "  total_hidden_states.append(list(output_t)) # 각시점의 은닉 상태의 값을 계속해서 축적\n",
        "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
        "\n",
        "print(total_hidden_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIzZOwVHYBI4",
        "outputId": "64aa1a0f-6e06-43a6-c769-aca9922d0401"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "(6, 8)\n",
            "(7, 8)\n",
            "(8, 8)\n",
            "(9, 8)\n",
            "(10, 8)\n",
            "[[0.97473801 0.80855673 0.85124951 0.94741352 0.80719304 0.89963183\n",
            "  0.7915908  0.77092437]\n",
            " [0.99964888 0.99973791 0.99991663 0.99963418 0.99964201 0.9999096\n",
            "  0.99976492 0.99962991]\n",
            " [0.99987899 0.99996783 0.99997496 0.99986461 0.99998069 0.99999\n",
            "  0.99994807 0.99990758]\n",
            " [0.99997648 0.99998316 0.99999053 0.99997142 0.99999208 0.99999701\n",
            "  0.99998259 0.9999766 ]\n",
            " [0.9999379  0.99996891 0.99997728 0.99995194 0.99997336 0.99998776\n",
            "  0.99997281 0.99991717]\n",
            " [0.99995921 0.99997857 0.99998958 0.99994593 0.99996943 0.99999075\n",
            "  0.9999716  0.99997652]\n",
            " [0.99993138 0.99994582 0.99997801 0.99993655 0.99994972 0.99998609\n",
            "  0.99996169 0.99992041]\n",
            " [0.99994843 0.99997187 0.99998265 0.99994446 0.9999895  0.99999547\n",
            "  0.99997094 0.99994105]\n",
            " [0.99998254 0.99998412 0.99998937 0.99998751 0.99997581 0.99999079\n",
            "  0.99998967 0.99997386]\n",
            " [0.99990147 0.99995877 0.9999822  0.99986255 0.99995427 0.9999867\n",
            "  0.99994117 0.99994716]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치의 nn.RNN()\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 5 # 입력의 크기\n",
        "hidden_size = 8 # 은닉 상태의 크기\n",
        "\n",
        "# 입력 텐서 정의\n",
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "# batch_first=True -> 입력 텐서의 첫번째 차원이 배치 크기임을 알려줌\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "# outputs: 모든 시점의 은닉 상태들\n",
        "# _status: 마지막 시점의 은닉 상태\n",
        "outputs, _status = cell(inputs)\n",
        "\n",
        "print(outputs.shape) # 모든 time-step의 hidden_state\n",
        "print(_status.shape) # 최종 time-step의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1mLOnvoYoLx",
        "outputId": "11b6c686-35c7-4824-8e78-9f1c15063ec2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 깊은 순환 신경망\n",
        "# 깊은 순환 신경망을 파이토치로 구현할때는 nn.RNN()의 인자인 num_layers에 값을 전달하여 층을 쌓음.\n",
        "# 입력 데이터\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
        "outputs, _status = cell(inputs)\n",
        "print(outputs.shape)\n",
        "print(_status.shape) # (층의 개수, 배치 크기, 은닉 상태의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ7O69_lZTmd",
        "outputId": "998a2c0b-977b-4be9-c4c9-38c1f604a4d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([2, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 양방향 순환 신경망\n",
        "# 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반\n",
        "# 양방향 RNN은 하나의 출력값을 예측하기위해 기본적으로 두 개의 메모리 셀을 사용\n",
        "# 첫번째 메모리 셀은 앞에서 배운 것처럼 앞 시점의 은닉 상태를 전달받아 현재의 은닉 상태 계산\n",
        "# 두번째 메모리 셀은 뒤 시점의 은닉 상태를 전달 받아 현재의 은닉 상태를 계산\n",
        "# 이 두 개의 값 모두가 출력층에서 출력값을 예측하기 위해 사용된다.\n",
        "# nn.RNN()의 인자인 bidirectional에 값을 True로 전달하면 된다.\n",
        "\n",
        "# 학습 데이터\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "cell = nn.RNN(input_size=5, hidden_size=8, num_layers=2, batch_first=True, bidirectional=True)\n",
        "outputs, _status = cell(inputs)\n",
        "print(outputs.shape) # (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2)\n",
        "print(_status.shape) # (층의 개수 * 2, 배치 크기, 은닉 상태의 크기)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUAS8Y71bUXv",
        "outputId": "4645630f-baf6-46e7-e3a6-255191cd2b60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6-2 LSTM과 GRU\n",
        "\n",
        "## 1. 바닐라 RNN의 한계\n",
        "- 바닐라 RNN은 비교적 짧은 시퀀스에 대해서만 효과를 보이는 단점이 있다.\n",
        "- 바닐라 RNN의 시점이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생한다.\n",
        "- 이를 **장기 의존성 문제**라고 한다.\n",
        "\n",
        "## 2. LSTM(Long Short-Term Memory)\n",
        "- LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것을 정한다.\n",
        "- LSTM은 은닉 상태를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 셀 상태라는 값을 추가했다.\n",
        "- LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보인다.\n",
        "- 셀 상태 또한 은닉 상태처럼 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로서 사용된다.\n",
        "- 은닉 상태값과 셀 상태값을 구하기 위해서 새로 추가 된 3개의 게이트를 사용한다. 각 게이트는 삭제 게이트, 입력 게이트, 출력 게이트라고 부르며 이 3개의 게이트에는 공통적으로 시그모이드 함수가 존재한다.\n",
        "- 시그모이드 함수를 지나면 0과 1사이의 값이 나오게 되는 이 값들을 가지고 게이트를 조절한다.\n",
        "- 1) 입력 게이트\n",
        "  - $i_t = σ(W_{xi}x_t + W_{hi}h_{t-1} + b_i)$\n",
        "  - $g_t = tanh(W_{xg}x_t + W_{hg}h_{t-1} + b_g)$\n",
        "  - 입력 게이트는 현재 정보를 기억하기 위한 게이트이다.\n",
        "  - 현재 시점 t의 x값과 입력 게이트로 이어지는 가중치 $W_{xi}$를 곱한 값과 이전 시점의 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치 $W_{hi}$를 곱한 값을 더하여 시그모이드 함수를 지난다. 이를 $i_t$라고 한다.\n",
        "  - 현재 시점 t의 x값과 입력 게이트로 이어지는 가중치 $W_{xi}$를 곱한 값과 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치 $W_{hg}$를 곱한 값을 더하여 하이퍼볼릭탄젠트 함수를 지난다. 이를 $g_t$라고 한다.\n",
        "  - 시그모이드 함수를 지나 0과 1 사이의 값과 하이퍼볼릭탄젠트 함수를 지나 -1과 1 사이의 값 두개가 나오게 된다. 이 두 개의 값을 가지고 이번에 선택된 기억할 정보의 양을 정한다.\n",
        "- 2) 삭제 게이트\n",
        "  - $f_t = σ(W_{xf}x_t + W_{hf}h_{t-1} + b_f)$\n",
        "  - 삭제 게이트는 기억을 삭제하기 위한 게이트이다\n",
        "  - 현재시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지나 0과 1 사이의 값이 나오게 되는데, 이 값이 삭제 과정을 거친 정보의 양이다.\n",
        "  - 0에 가까울수록 정보가 많이 삭제된 것이고 1에 가까울수록 정보를 온전히 기억한 것이다.\n",
        "- 3) 셀 상태(장기 상태)\n",
        "  - $C_t = f_t∘C_{t-1} + i_t∘g_t$\n",
        "  - 셀 상태 $C_t$를 LSTM에서는 장기 상태라고 부르기도 한다.\n",
        "  - 입력 게이트에서 구한 $i_t, g_t$ 이 두 개의 값에 대해서 원소별 곲을 진행한다.\n",
        "  - 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더한다. 이 값을 현재 시점 t의 셀 상태라고 하며, 이 값은 다음 t+1 시점의 LSTM 셀로 넘겨진다.\n",
        "- 4) 출력 게이트와 은닉 상태(단기 상태)\n",
        "  - $o_t = σ(W_{xo}x_t + W_{ho}h_{t-1} + b_o)$\n",
        "  - $h_t = o_t∘tanh(c_t)$\n",
        "  - 출력 게이트는 현재 시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값이다.\n",
        "  - 해당 값은 현재 시점 t의 은닉 상태를 결정하는 일에 쓰인다\n",
        "  - 은닉 상태를 단기 상태라고도 한다. 은닉 상태는 장기 상태의 값이 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값이다.\n",
        "  - 해당 값은 출력 게이트의 값과 연산되면서, 값이 걸러지는 효과가 발생한다.\n",
        "  - 단기 상태의 값은 또한 출력층으로도 향한다.\n",
        "\n",
        "## 3. 파이토치의 nn.LSTM()\n",
        "- 기존 RNN 사용 방법과 유사하게 사용하면 된다.\n",
        "- `nn.LSTM(input_dim, hidden_size, batch_first=True)`\n",
        "- GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였다. 즉, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰다\n",
        "\n",
        "## 4. GRU(Gated Recurrent Unit)\n",
        "- GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재한다.\n",
        "- $r_t = σ(W_{xr}x_t + W_{hr}h_{t-1} + b_r)$\n",
        "- $z_t = σ(W_{xz}x_t + W_{hz}h_{t-1} + b_z)$\n",
        "- $g_t = tanh(W_{hg}(r_t∘h_{t-1}) + W_{xg}x_t + b_g)$\n",
        "- $h_t = (1 - z_t)∘g_t + z_t∘h_{t-1}$\n",
        "- GRU와 LSTM 중 어떤 것이 모델의 성능면에서 더 낫다라고 단정지어 말할 수 없으며, 기존에 LSTM을 사용하면서 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU로 바꿔서 사용할 필요는 없다.\n",
        "- 경험적으로 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 많으면 LSTM이 더 낫다고 한다.\n",
        "\n",
        "## 5. 파이토치의 nn.GRU()\n",
        "- `nn.GRU(input_dim, hidden_size, batch_first=True)`"
      ],
      "metadata": {
        "id": "JtCZLDR7gOmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6-3 문자 단위 RNN(Char RNN): 실습\n",
        "- RNN의 입출력의 단위가 단어 레벨이 아니라 문제 레벨로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 한다."
      ],
      "metadata": {
        "id": "PjTfZErOtv8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# 1) 훈련 데이터 전처리 하기\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str + label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgJUwZgikOgw",
        "outputId": "893e4f14-9118-45db-bffa-9c4c3e6a033a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기(입력은 원-핫 벡터 사용)\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)\n",
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "  index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iIFnKIQuiJV",
        "outputId": "29a165e4-0139-4b76-ffc9-7e73f63c2023"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QStINl0zu_ta",
        "outputId": "fe8e31dd-604a-4ae5-889f-6a3030fe5b87"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.RNN()은 기본적으로 3차원 텐서를 입력 받는다.\n",
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)을 통해 해결할 수도 있음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D92XZdpLvXsT",
        "outputId": "2dc48944-fe45-4ee2-e05c-0350e69958a5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔준다\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yutRW-VSvkEN",
        "outputId": "5fce5875-0064-4350-9c42-79563fe8130d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY3tdk-GvuLc",
        "outputId": "3cd4b93f-673c-4af1-df9a-e5da1e3a17a1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) 모델 구현하기\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "net = Net(input_size, hidden_size, output_size)\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTlurOahwN6X",
        "outputId": "ddf78182-8a29-419c-b03f-80464a3d7eea"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # Batch 차원 제거를 위해\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 아래 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드\n",
        "  result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은값의 인덱스 선택\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i, 'loss: ', loss.item(), 'prediction: ', result, 'true Y:', y_data, 'prediction str: ', result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v21wx7uEwzSe",
        "outputId": "465783b2-ffb4-4911-ac58-98b8da24f7c7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.5643374919891357 prediction:  [[4 4 4 2 4]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
            "1 loss:  1.2644165754318237 prediction:  [[4 4 0 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pp!e!\n",
            "2 loss:  1.0176327228546143 prediction:  [[4 4 4 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "3 loss:  0.7850898504257202 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "4 loss:  0.5807846784591675 prediction:  [[4 4 4 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.4233388900756836 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.30705526471138 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.22082757949829102 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.15697558224201202 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.11162887513637543 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.08033889532089233 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.05920081213116646 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.04487527534365654 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.03488669544458389 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.02765120565891266 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.022231431677937508 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.018081050366163254 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.01486736349761486 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.012366748414933681 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.010413491167128086 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.008879045024514198 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.007663370575755835 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.006689299829304218 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.005897949915379286 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0052445135079324245 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.004695492796599865 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.004226663149893284 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.003820971120148897 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.003467019647359848 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0031571988947689533 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.002886256668716669 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0026498869992792606 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0024444793816655874 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.002266646595671773 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.002112752292305231 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0019796520937234163 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0018643394578248262 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0017641354352235794 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.001676739426329732 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.001600205316208303 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0015329427551478148 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0014734547585248947 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.001420672982931137 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0013734560925513506 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0013312817318364978 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0012933179968968034 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0012589938705787063 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0012278815265744925 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0011995533714070916 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.001173652010038495 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0011498687090352178 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0011280126636847854 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0011078228708356619 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0010891321580857038 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0010717746336013079 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0010555831249803305 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0010404152562841773 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0010262234136462212 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0010128645226359367 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.001000195974484086 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0009883369784802198 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0009770251344889402 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0009662850643508136 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.000956044823396951 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0009463282185606658 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0009369212202727795 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0009279664373025298 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0009193450096063316 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.000911033246666193 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.000903006992302835 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0008952427888289094 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0008877404034137726 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0008804522221907973 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0008733784779906273 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0008664950728416443 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0008597783744335175 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0008532522479072213 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0008467735606245697 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.000840580731164664 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0008344355155713856 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.000828433025162667 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0008225021883845329 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0008167140185832977 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0008110211347229779 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0008053996716625988 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0007998973014764488 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0007945139659568667 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0007891306886449456 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0007838663877919316 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.000778649584390223 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0007735757972113788 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0007684543961659074 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0007634519715793431 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0007584734121337533 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0007536138291470706 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.000748754246160388 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0007439661421813071 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0007392256520688534 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0007345803314819932 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0007299351273104548 prediction:  [[4 4 3 2 0]] true Y: [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 더 많은 데이터로 학습한 문자 단위 RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1) 훈련 데이터 전처리하기\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic)\n",
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO-Q-w_Px5tV",
        "outputId": "1bab5b7d-7528-4e31-cd14-709a543b8e2c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'y': 0, 'm': 1, 't': 2, 'b': 3, 'l': 4, 'w': 5, 'f': 6, \"'\": 7, 'a': 8, 'p': 9, '.': 10, 'i': 11, 'k': 12, 'h': 13, 'n': 14, 'r': 15, 's': 16, 'c': 17, 'd': 18, 'u': 19, 'g': 20, 'e': 21, ' ': 22, 'o': 23, ',': 24}\n",
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10 # 임의 숫자 지정\n",
        "learning_rate = 0.1\n",
        "\n",
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence)- sequence_length):\n",
        "  x_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "  print(i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str])\n",
        "  y_data.append([char_dic[c] for c in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gSSJqPlyAcK",
        "outputId": "f562f3fe-b147-4eb5-9b96-a9f6784569f6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyt-AUWk48E0",
        "outputId": "c4fe23bb-204a-4cf4-b165-f92a74b1849f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 6, 22, 0, 23, 19, 22, 5, 8, 14]\n",
            "[6, 22, 0, 23, 19, 22, 5, 8, 14, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "UlrNQGuh5Ds9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))\n",
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGj4rtep8Vwt",
        "outputId": "f0a6c423-2c0a-4691-9772-52d7b03ae717"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([ 6, 22,  0, 23, 19, 22,  5,  8, 14,  2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구현하기\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2) # 층을 2개 쌓음\n",
        "\n",
        "# 비용 함수\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6CI-ryS8e8A",
        "outputId": "3e924ad8-1873-4095-a44b-61fb21b8aeb6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # results의 텐서 크기는 (170, 10)\n",
        "  results = outputs.argmax(dim=2)\n",
        "  predict_str = \"\"\n",
        "  for j, result in enumerate(results):\n",
        "    if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "      predict_str += char_set[result[-1]]\n",
        "\n",
        "  print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYnNdpSj9KkP",
        "outputId": "1c968f70-5349-4219-ab7b-1a11e8d7b4ff"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AMKp7j49uM2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}